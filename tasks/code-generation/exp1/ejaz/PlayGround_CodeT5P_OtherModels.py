# -*- coding: utf-8 -*-
"""Copy of CodeT5_Week2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U3Qfd0T6oqGQ7ZvV-NK7FT9hMWMqFHst
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torch torchvision torchaudio

from transformers import RobertaTokenizer, T5ForConditionalGeneration

tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')
model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')

text = "def greet(user): print(f'hello <extra_id_0>!')"
input_ids = tokenizer(text, return_tensors="pt").input_ids

# simply generate one code span
generated_ids = model.generate(input_ids, max_length=8)
print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))
# this prints "{user.username}"

# Define input text (prompt) for generating Python code to reverse a list
text = "Once upon a time in a"

# Tokenize input text
input_ids = tokenizer(text, return_tensors="pt").input_ids

# Generate code
generated_ids = model.generate(input_ids, max_length=100)
generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the generated Python code
print(generated_code)

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load pre-trained GPT-2 tokenizer and model
gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')

# Define input text (prompt)
text = "Once upon a time in a"

# Tokenize input text
input_ids = gpt_tokenizer.encode(text, return_tensors="pt")

# Generate text
generated_ids = gpt_model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)

# Decode and print the generated text
generated_text = gpt_tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(generated_text)

# Define input text (prompt)
text = "write python code to reverse a list"

# Tokenize input text
input_ids = gpt_tokenizer.encode(text, return_tensors="pt")

# Generate text
generated_ids = gpt_model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)

# Decode and print the generated text
generated_text = gpt_tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(generated_text)

from transformers import AutoModel, AutoTokenizer

checkpoint = "Salesforce/codet5p-110m-embedding"
device = "cuda"  # for GPU usage or "cpu" for CPU usage

tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)
model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)

inputs = tokenizer.encode("def print_hello_world():\tprint('Hello World!')", return_tensors="pt").to(device)
embedding = model(inputs)[0]
print(f'Dimension of the embedding: {embedding.size()[0]}, with norm={embedding.norm().item()}')
# Dimension of the embedding: 256, with norm=1.0

# Commented out IPython magic to ensure Python compatibility.
# %pip install sacrebleu

# Commented out IPython magic to ensure Python compatibility.
# %pip install rouge

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch
# from sacrebleu import corpus_bleu
# from rouge import Rouge

checkpoint = "Salesforce/codet5p-770m-py" #codet5p-2b codet5p-6b codet5p-16b codet5p-770m-py
device = "cuda" # for GPU usage or "cpu" for CPU usage
tokenizer = AutoTokenizer.from_pretrained (checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained (checkpoint,
torch_dtype=torch.float16,
#load_in_8bit=True,
#device_map="auto",
trust_remote_code=True).to (device)
print (model.get_memory_footprint())

# python_function ="Write python code for linear regression" # text-to-code
java_function ="public boolean isEven(int num){ //Complete"
encoding = tokenizer (java_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=300)
generated_code = tokenizer.decode (outputs[0], skip_special_tokens=True)

print(generated_code)

# Reference code (ground truth)
# reference_code = ["# Function to check if a number is prime",
#                   "def is_prime(num):",
#                   "    if num < 2:",
#                   "        return False",
#                   "    for i in range(2, int(num**0.5) + 1):",
#                   "        if num % i == 0:",
#                   "            return False",
#                   "    return True",
#                   "",
#                   "# Function to generate the first n prime numbers",
#                   "def generate_primes(count):",
#                   "    primes = []",
#                   "    num = 2",
#                   "    while len(primes) < count:",
#                   "        if is_prime(num):",
#                   "            primes.append(num)",
#                   "        num += 1",
#                   "    return primes",
#                   "",
#                   "# Print the first 10 prime numbers",
#                   "first_10_primes = generate_primes(10)",
#                   'print("First 10 Prime Numbers:", first_10_primes)']

# # BLEU Score using sacrebleu
# bleu_score_sacrebleu = corpus_bleu(generated_code, [reference_code])
# print("BLEU Score (sacrebleu):", bleu_score_sacrebleu.score)

java_function ="public int sumNumbers(int a, int b){ //Complete"
encoding = tokenizer (java_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=300)
generated_code = tokenizer.decode (outputs[0], skip_special_tokens=True)

print(generated_code)

java_function ="public int findMaximum(int a, int b){ //Complete"
encoding = tokenizer (java_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=300)
generated_code = tokenizer.decode (outputs[0], skip_special_tokens=True)

print(generated_code)

java_function ="public int factorial(int num){ //Complete"
encoding = tokenizer (java_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=300)
generated_code = tokenizer.decode (outputs[0], skip_special_tokens=True)

print(generated_code)

java_function ="public String reverseString(String input){ //Complete"
encoding = tokenizer (java_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=300)
generated_code = tokenizer.decode (outputs[0], skip_special_tokens=True)

print(generated_code)

python_function ="def fit_linear_regression(X,y):" # code completion


encoding = tokenizer (python_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=750)
print(tokenizer.decode (outputs[0], skip_special_tokens=True))

python_function ="def generate_primes(count):" # code completion


encoding = tokenizer (python_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=750)
generated_code = tokenizer.decode (outputs[0], skip_special_tokens=True)
print(generated_code)

# Reference code (ground truth)
reference_code = ["# Function to check if a number is prime",
                  "def is_prime(num):",
                  "    if num < 2:",
                  "        return False",
                  "    for i in range(2, int(num**0.5) + 1):",
                  "        if num % i == 0:",
                  "            return False",
                  "    return True",
                  "",
                  "# Function to generate the first n prime numbers",
                  "def generate_primes(count):",
                  "    primes = []",
                  "    num = 2",
                  "    while len(primes) < count:",
                  "        if is_prime(num):",
                  "            primes.append(num)",
                  "        num += 1",
                  "    return primes",
                  "",
                  "# Print the first 10 prime numbers",
                  "first_10_primes = generate_primes(10)",
                  'print("First 10 Prime Numbers:", first_10_primes)']

# BLEU Score using sacrebleu
bleu_score_sacrebleu = corpus_bleu(generated_code, [reference_code])
print("BLEU Score (sacrebleu):", bleu_score_sacrebleu.score)

python_function ="Write python code for SBERT vector embedding of a sentence" # text-to-code

encoding = tokenizer (python_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
outputs = model.generate(**encoding, max_length=750)
print(tokenizer.decode (outputs[0], skip_special_tokens=True))

python_function ="Write python code for SBERT vector embedding of a sentence" # text-to-code

encoding = tokenizer (python_function, return_tensors="pt").to(device)
encoding['decoder_input_ids'] = encoding['input_ids'].clone()
print(encoding)
outputs = model.generate(**encoding, max_length=750)
print(outputs)
print(tokenizer.decode (outputs[0], skip_special_tokens=True))

from transformers import RobertaTokenizer, T5ForConditionalGeneration

# Load tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("Salesforce/codet5-base")
model = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5-base")

# Your code snippet here
code_snippet = """
def add(a, b):
    return a + b
"""

# Prefixing the input with the task you want to perform
input_text = "summarize: " + code_snippet

# Tokenize and encode the input
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate the summary
output = model.generate(input_ids)
summary = tokenizer.decode(output[0], skip_special_tokens=True)

print("Generated Summary:", summary)