# How to Run?
- Run CodeT5P.py on Colab to get the output predictions.
- Update predictions and prompts in predictions.txt and answers.json respectively
- Run eval.py to get Bleu and EM scores.
  - `python eval.py -a answers.json -p predictions.txt`

# Analysis and Open Questions
- *Are we correctly evaluating the code generated models ?*
  - Evaluating code generated by models presents a challenge. Common metrics like Bleu, Rouge, etc., designed for text, might not be optimal for code evaluation.
  - While comparing with the best-written code has its merits, it might impose limitations and not account for diverse requirements.
  - Relying solely on the best-written code might restrict the model's ability to handle diverse coding scenarios and edge cases.

  - *What I think:*
    - Testing with a range of edge cases is crucial. Generative models should also generate a broad spectrum of test cases along with generated code.
    - When both test cases and generated code are available we can use a system(such as the Judge0 API) to evaluate the generated code against these test cases and provide valuable feedback for model improvement.
 
- *When to use encoder and decoder model and why ?*
    - I learned that the encoder processes the input and extracts similarities and other information in the text. Then this information is utilized by the decoder to generate responses.
    - Different language models employ varying architectures based on their specific tasks. BERT, using only an encoder, focuses on context understanding. GPT-2, a decoder model, excels in language generation. Code T5 leverages both encoder and decoder, striking a balance between understanding and generating text.
    - All the above models are used to generate text. I'm not able to understand how a specific architecture model is benificial in what scenarios, when should one choose the correct architecture?
 
- *Why is CodeT5P inclined towards python language ?*
  - when working with code t5 plus, even though when the prompt specifically asks for java code, sometimes the ouput generated is in python.
  - Do we need to have different versions of code generation models specific to a language ? to increase their accuracy ? or does generalization, the opposite increases a models accuracy ?
 
# Output of eval.py
- `INFO:__main__:BLEU: 35.87, EM: 20.0`
- BLEU: 35.87 (higher is better, with 100 being a perfect match)
- EM: 20.0% (percentage of exact matches)





